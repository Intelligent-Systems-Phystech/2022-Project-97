\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}


\title{Anti-Distillation: Knowledge Transfer from Simple Model to a Complex One}

\author{\textbf{Kseniia Petrushina,~Andrey Grabovoy,~Oleg Bakhteev,~Vadim Strijov} \\
	Moscow Institute of Physics and Technology \\
	\texttt{\{petrushina.ke,~grabovoy.av,~bakhteev,~strijov\}@phystech.edu}
}
\date{}
\renewcommand{\undertitle}{}
\renewcommand{\headeright}{}
\renewcommand{\shorttitle}{Anti-Distillation: Knowledge Transfer from Simple Model to a Complex One}

\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	This paper considers the problem of adapting the model to new data with a large amount of information. We propose to construct a complex model with further knowledge transfer from a simple model to it. It is necessary to take into account not only the quality of the prediction on the original samples but also the adaptability to new data and the robustness of the obtained solution. The novelty of the work lies in the fact that our method allows adapting the pre-trained model to a more heterogeneous dataset. This study considers both probabilistic and algebraic methods for obtaining a new model. In the computational experiment, we analyse the quality of predictions on synthetic and natural samples. FashionMnist and CIFAR10 datasets are our sources of real-world data.
\end{abstract}

\keywords{Distillation \and Knowledge Transfer \and Weight Initialization \and Machine Learning}

\section{Introduction}
In the modern world, the number of machine learning tasks grows rapidly, as is the amount of data to be processed. Typically, training a model from scratch can lead to poor results or take a long time. To get better results faster, researchers have been developing various methods allowing to use of existing trained models to solve new problems. For instance, there are knowledge distillation \citep{hinton2015distilling, lopezpaz2016unifying}, transfer learning \citep{zhuang2019acomprehensive}, fine-tuning, low-rank model approximation \citep{yu2017oncompressing}. Moreover, scientists have studied methods for initializing model weights for faster convergence \citep{glorot2010understanding}. These approaches help to decrease the time needed for training or inference and achieve higher quality. 

Consider the distillation method in more detail. Statement of the initial problem is the transfer of knowledge from a cumbersome neural network or ensemble of ones to a smaller model in the classification task. Hinton and others were able to achieve this by training the student model to reproduce the probability distribution of the classes produced by the teacher model. The use of such soft targets helped to carry more information, so the student models generalization ability is comparable to the teachers. However, this research focuses on model compression under conditions of input data persistence. And we want to solve the inverse problem: in a sense to keep the model under conditions of increasing sample complexity.

Thus, the novelty of this work lies in the proposal of a method for increasing the complexity of the model based on a pre-trained one. This is done by growing the dimension of the weight space and initializing part of the student neural network with teacher model weights. We propose that our approach allows us to speed up neural network training and obtain a more stable solution. As the learning is supposed to start close to the optimum point. In this way, we can adapt the pre-trained model to more variable data and reuse previously learned information.

This paper presents computational experiments on various ways of complicating the model. We consider fully connected, convolutional layers and LSTM modules. An experiment is to compare uniform initialization with one based on a previously trained model and analyse differences in convergence rate, prediction variance, and achieved quality.

\section{Setup}
We focus on $c$-class classification, although the same ideas apply to other machine learning tasks. Consider two samples
$$S_1 = \{(x_i, y_i)\}_{i=1}^{N_1},~x_i \in \mathbb{R}^{d_1},~y_i \in \Delta^{c_1},$$
$$S_2 =  \{(x_j', y_j')\}_{j=1}^{N_2},~x_j' \in \mathbb{R}^{d_2},~y_j' \in \Delta^{c_2},$$

where $\Delta^c$ is the set of $c$-dimentional probability vectors.

Having a teacher model $f_{tr}: \mathbb{R}^{d_1} \rightarrow \Delta^{c_1},~f_{tr}(x) = f(x, w^*)$, where weights of the model are defined as follows:
$$w^* =  \underset{w}{\arg\min}~\mathcal{L}_w =\underset{w}{\arg\min}~\sum\limits_{i=1}^{N_1} l (y_i,~f(x_i, w)),$$
here, $l$ is a cross-entropy loss 
$$l(y, \hat{y}) = -\sum\limits_{k=1}^{c} y_k \log{\hat{y}_k},~y \in \Delta^c,$$

our approach proposes constructing student model $f_{st}: \mathbb{R}^{d_2} \rightarrow \Delta^{c_2},~f_{st}(x) = g(x, u^*)$,

$$u^* =  \underset{u}{\arg\min}~\mathcal{L}_u = \underset{u}{\arg\min}~\sum\limits_{j=1}^{N_2} l (y_j',~g(x_j', u)).$$

We find the solution to the above optimization problem using gradient optimization methods. Model weights update as
$$u_{t+1} = T(u_t | \mathcal{L}_u, S_2),$$
where $T: \mathbb{R}^{|u|} \rightarrow \mathbb{R}^{|u|}$ is the optimization operator and $t \in \mathbb{N}$ is the gradient step number.

In this case, function $\varphi: \mathbb{R}^{|w|} \rightarrow \mathbb{R}^{|u|}$ determines student model initialization $u_1 = \varphi(w^*)$.

\bibliographystyle{plain}
\bibliography{Petrushina2022AntiDistillation.bib}

\end{document}