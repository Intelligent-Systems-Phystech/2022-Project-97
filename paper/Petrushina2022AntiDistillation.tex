\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}


\title{Anti-Distillation or Teacher Learning: Knowledge Transfer from Simple Model to a Complex One}

\author{ Kseniia ~Petrushina \\
	Moscow Institute of Physics and Technology\\
	Dolgoprudny, Russia \\
	\texttt{petrushina.ke@phystech.edu} \\
	%% examples of more authors
	\And
	Andreii Hraboviy \\
	Moscow Institute of Physics and Technology\\
	Dolgoprudny, Russia \\
	\texttt{grabovoy.av@phystech.edu} \\
	%% examples of more authors
	\And
	Oleg Bakhteev \\
	Moscow Institute of Physics and Technology\\
	Dolgoprudny, Russia \\
	\texttt{bakhteev@phystech.edu}
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	We are considering the problem of adapting the model to a new data with a large amount of information. It is proposed to build a model of greater complexity with further knowledge transfer from a simple model to it. It is necessary to take into account not only the quality of the prediction on the original samples, but also the adaptability to novel data and the robustness of the obtained solution.
\end{abstract}


\keywords{Anti-Distillation \and Distillation \and Knowledge Transfer \and Weight Initialization}

\section{Introduction}


\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}